{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression: A Probabilistic, Discriminative Model\n",
    "\n",
    "Logistic regression, a discriminative model, is used for classification decisions in which the output is binary. Specifically, its model is that the probability of the event of interest occuring is Bernoulli with $\\mu_i=\\sigma(w^tx_i)$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(y_i|x_i,w) &= Ber(y_i|\\sigma(w^Tx_i)) \\\\\n",
    "             &= (\\sigma(w^Tx))^{I(y_i=1)} (1 - \\sigma(w^Tx))^{I(y_i=0)}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with other methods, we can first consider the MLE $\\hat{w}$. To begin, we write out the negative log-likelihood of the data:\n",
    "$$\n",
    "\\begin{align}\n",
    "    NLL(w) &= -log\\ p(y|x, w) \\\\\n",
    "           &= -log \\prod_{i=1}^N p(y_i|x_i,w) \\\\\n",
    "           &= - \\sum_{i=1}^N log[p(y_i|x_i,w)] \\\\\n",
    "           &= - \\sum_{i=1}^N y_i\\ log \\mu_i + (1-y_i)log(1 - \\mu_i) \\\\\n",
    "           &= - \\sum_{i=1}^N y_i\\ log( \\lbrace \\frac {e^{w^Tx_i}} {1 + e^{w^Tx_i}} \\rbrace ) + (1-y_i)\\ log(1 - \\lbrace \\frac {e^{w^Tx_i}} {1 + e^{w^Tx_i}} \\rbrace )\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "You may recognize the last expression as standard \"cross-entropy\" loss. Either way, you can now also remember this expression as being the negative log-likelihood of a sequence $\\{y_i\\}$ of $N$ Bernoulli random variables which follow $Ber(y|\\sigma(w^Tx))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just need to solve for the $w$ that minimizes this negative log-likelihood. The problem here is that, unlike in the linear regresssion case, there is no closed-form solution $\\hat{w}$ -- we cannot analytically write down what $\\hat{w}$ is in terms of the other variables. However, what we can do is define what the gradient (and Hessian) are for $NLL(w)$ and _iterate_ towards the solution. Additionally, since this $NLL(w)$ is convex, iterating in the opposite direction of the gradient will (eventually) reach the global minimum, and accordingly yield the optimal $\\hat{w}$. Be careful to not assume that, just because the algorithm is iterative, it is solving a non-convex problem. The problem is indeed convex, but requires iterations, instead of a single equation solve.\n",
    "\n",
    "The gradient and Hessian of $NLL(w)$ are shown below:\n",
    "$$\n",
    "\\begin{align}\n",
    "    g &= \\frac d {dw} NLL(w) \\\\\n",
    "      &= \\sum_{i=1}^N (\\mu_i-y_i)x_i \\\\\n",
    "      &= X^T(\\mu - y)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    H &= \\frac d {dw} g \\\\\n",
    "      &= \\sum_{i=1}^N (\\nabla_w \\mu_i)x^T_i \\\\\n",
    "      &= \\sum_{i=1}^N \\mu_i(1 - \\mu_i)x^T_i \\\\\n",
    "      &= X^TSX,\\ S=diag(\\mu_i(1-\\mu_i))\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class Logistic Regression\n",
    "We know that the likelihood of a binary response variable for logistic regression modeling is a Bernoulli likelihood. Now let's generalize this to the multi-class case, where the response variable becomes Multinoulli:\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(y=c|x,W) &= \\frac {e^{w^T_cx}} {\\sum_{c'=1}^C e^{w^T_{c'}x}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aside: Gaussian Approximation\n",
    "Sometimes, it is convenient to approximate a distribution with a class of functions that are more computationally feasible than the target distribution itself. This will be the case next, when we look at Bayesian logistic regression. One such approximation is known as the \"Gaussian Approximation\" or \"Laplace Approximation\".\n",
    "\n",
    "Suppose we wish to approximate the posterior distribution of an arbitrary parameter vector $\\theta$ in $R^D$. In this case, let\n",
    "$$\n",
    "\\begin{align}\n",
    "    p(\\theta|D) &= \\frac 1 Z e^{-E(\\theta)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We know that we want to use a Gaussian to approximate the posterior distribution. We also know that the logarithm of the Gaussian has a quadratic (second-order) relationship in $x$. Because of this, we can arrive at an approximation to the distribution by using its Taylor series expansion out to the second-order term. By choosing a mode $\\theta^*$ as the point to compute the Taylor series expansion from, the gradient term is zero.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    E(\\theta) &\\approx E(\\theta^*) + (\\theta-\\theta^*)^Tg + \\frac 1 2 (\\theta-\\theta^*)^T H (\\theta-\\theta^*) \\\\\n",
    "              &= E(\\theta^*) + \\frac 1 2 (\\theta-\\theta^*)^T H (\\theta-\\theta^*) \\\\\n",
    "              \\\\\n",
    "   \\Rightarrow \\hat{p}(\\theta|D) &= \\frac 1 Z e^{E(\\theta^*)} e^{-\\frac 1 2(\\theta-\\theta^*)^T H (\\theta-\\theta^*)} \\\\\n",
    "                                 &= N(\\theta|\\theta^*, H^{-1}) \\\\\n",
    "                               Z &= p(D) \\approx \\int \\hat{p}(\\theta|D)d\\theta = e^{-E(\\theta^*)}(2\\pi)^{D/2} |H|^{- \\frac 1 2} \\\\\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Logistic Regression\n",
    "Just like in linear regression, we can consider the _distribution_ of $\\hat{w}$, as opposed to just a point estimate. However, unlike in linear regression, we cannot compute the posterior exactly. This is due to the fact that there is no mathematically convenient conjugate prior for the Bernoulli likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
