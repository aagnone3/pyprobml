{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ermongroup.github.io/cs228-notes/inference/variational/\n",
    "\n",
    "https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1601.00670.pdf\n",
    "\n",
    "## Idea\n",
    "Pick an approximation $q(x)$ to the distribution from a *tractable* family, and make the approximation $q(x)$ be as close as possible to the true distribution $p^*(x)=p(x|D)$.\n",
    "\n",
    "Note: a cheap alternative is a Gaussian approximation; however, this is only valid when the true distribution $p(x)$ is well-modeled by a Gaussian distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI\n",
    "Throughout the derivation, remember that $p^*(x)$ is intractable to compute.\n",
    "\n",
    "First thought: minimize the KL divergence of $p^*$ from q:\n",
    "$$KL(p^*||q)=\\sum_x p^*(x) log \\frac {p^*(x)} {q(x)}$$\n",
    "\n",
    "However, the first term is intractable, since it is an expectation of $p^*$. So let's use the reverse KL:\n",
    "$$KL(q||p^*)=\\sum_x q(x) log \\frac {q(x)} {p^*(x)}$$\n",
    "\n",
    "Now, the expectation is on $q(x)$, which is tractable to compute. However, note the following:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p^*(x) &= p(x|D) \\\\\n",
    "       &= \\frac {p(x,D)} {p(D)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here, the normalization constant $Z=p(D)$ usually also intractable to compute. So we can't use this as-is. What we *can* usually reasonably compute is the un-normalized representation of $p^*(x)$, which is $p(x,D)=p^*(x)Z$.\n",
    "\n",
    "We then have the following optimization objective:\n",
    "$$\n",
    "\\begin{align}\n",
    "J(q) &= KL(q||\\tilde{p}) \\\\\n",
    "     &= \\sum_x p(x) log \\frac {q(x)} {\\tilde{p}} \\\\\n",
    "     &= \\sum_x p(x) log \\frac {q(x)} {p^*(x)Z} \\\\\n",
    "     &= \\sum_x p(x) log \\frac {q(x)} {p^*(x)} - logZ \\\\\n",
    "     &= KL(q||p^*) - logZ\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that we are now optimizing our previous reverse KL divergence, but with an additional constant $logZ$.\n",
    "\n",
    "Additionally, recall that $logZ=log\\ p(D)$. Since KL divergence is always non-negative, $KL(q||p^*)-logZ \\ge - log\\ p(D)$. What this shows is that the optimization objective value that we achieve is an upper bound on the NLL of the data. Equivalently, it is a lower bound on the log likelihood of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
